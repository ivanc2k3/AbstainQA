{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# --- 1. Linear Probe Model ---\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        self.out = nn.Linear(input_dim, 2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(self.activation(self.linear(x)))\n",
    "\n",
    "def train_linear_model(model, train_data, train_labels, epochs=10, batch_size=8, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    # Safety checks for tensor types\n",
    "    if not isinstance(train_data, torch.Tensor):\n",
    "        train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "    if not isinstance(train_labels, torch.Tensor):\n",
    "        train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "        \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        permutation = torch.randperm(train_data.size()[0])\n",
    "        \n",
    "        # --- NEW: Variables to track loss ---\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_data = train_data[indices]\n",
    "            batch_labels = train_labels[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # --- NEW: Accumulate the error ---\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "        # --- NEW: Print the average loss for this epoch ---\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        # tqdm.write prints above the progress bar so it doesn't get messy\n",
    "        tqdm.write(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def test_linear_model(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_data)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        # Return: (Predicted Class 0 or 1, Confidence of Class 0 (Abstain))\n",
    "        # Note: If Class 0 is \"Correct\", Class 1 is \"Wrong\". \n",
    "        # But typically for AbstainQA: \n",
    "        # We train to predict \"Is it Correct?\". So Class 1 = Correct, Class 0 = Wrong.\n",
    "        # Abstain Score usually relates to probability of being Wrong (Class 0).\n",
    "        return torch.argmax(outputs, dim=1), probs[0][0].item() \n",
    "\n",
    "# --- 2. Metrics Calculation ---\n",
    "def compute_metrics(correct_flags, abstain_flags, abstain_scores = None):\n",
    "    # correct_flags: a list of [0,1]s representing the correctness of each QA answered by the LLM\n",
    "    # abstain_flags: a list of [0,1]s representing whether the LLM abstained from answering each QA\n",
    "    # abstain_scores: a list of floats from 0 to 1 representing the confidence of the LLM in abstaining\n",
    "    # returns: a dictionary of metrics\n",
    "\n",
    "    assert len(correct_flags) == len(abstain_flags)\n",
    "\n",
    "    # group A: answered and correct\n",
    "    # group B: abstained and correct\n",
    "    # group C: answered and incorrect\n",
    "    # group D: abstained and incorrect\n",
    "    A = 0\n",
    "    B = 0\n",
    "    C = 0\n",
    "    D = 0\n",
    "    for i in range(len(correct_flags)):\n",
    "        if abstain_flags[i]:\n",
    "            if correct_flags[i]:\n",
    "                B += 1\n",
    "            else:\n",
    "                D += 1\n",
    "        else:\n",
    "            if correct_flags[i]:\n",
    "                A += 1\n",
    "            else:\n",
    "                C += 1\n",
    "        \n",
    "    # reliable accuracy: accuracy of the LLM on the questions it answered\n",
    "    try:\n",
    "        reliable_accuracy = A / (A + C)\n",
    "    except:\n",
    "        reliable_accuracy = None\n",
    "\n",
    "    # effective reliability: correct 1, incorrect -1, abstained 0\n",
    "    effective_reliability = (A - C) / (A + B + C + D)\n",
    "\n",
    "    # abstain accuracy: accuracy of the LLM abstain decisions, how many times correct_flags == !abstain flags\n",
    "    abstain_accuracy = (A + D) / (A + B + C + D)\n",
    "\n",
    "    # abstain precision: how many abstains is right among all abstains\n",
    "    try:\n",
    "        abstain_precision = D / (B + D)\n",
    "    except:\n",
    "        abstain_precision = None\n",
    "\n",
    "    # abstain recall: how many abstains is right among all incorrect answers\n",
    "    try:\n",
    "        abstain_recall = D / (C + D)\n",
    "    except:\n",
    "        abstain_recall = None\n",
    "\n",
    "    # abstain ECE: bucket abstain confidence into 10 buckets (0:0.1:1), compute the expected calibration error\n",
    "    if abstain_scores is not None and max(abstain_scores) != min(abstain_scores):\n",
    "\n",
    "        # rescale abstain scores to 0-1 before calculation\n",
    "        max_score = max(abstain_scores)\n",
    "        min_score = min(abstain_scores)\n",
    "        for i in range(len(abstain_scores)):\n",
    "            abstain_scores[i] = (abstain_scores[i] - min_score) / (max_score - min_score)\n",
    "\n",
    "        bucket_probs = [[] for i in range(10)]\n",
    "        bucket_abstain = [[] for i in range(10)] # whether it should have abstained\n",
    "\n",
    "        for i in range(len(abstain_scores)):\n",
    "            if abstain_scores[i] == 1:\n",
    "                bucket = 9\n",
    "            else:\n",
    "                bucket = int(abstain_scores[i] * 10)\n",
    "            bucket_probs[bucket].append(abstain_scores[i])\n",
    "            if correct_flags[i] == 1:\n",
    "                bucket_abstain[bucket].append(0)\n",
    "            else:\n",
    "                bucket_abstain[bucket].append(1)\n",
    "            \n",
    "        bucket_ece = 0\n",
    "        for i in range(10):\n",
    "            if len(bucket_probs[i]) == 0:\n",
    "                continue\n",
    "            bucket_probs_avg = np.mean(bucket_probs[i])\n",
    "            bucket_abstain_avg = np.mean(bucket_abstain[i])\n",
    "            bucket_ece += abs(bucket_abstain_avg - bucket_probs_avg) * len(bucket_probs[i])\n",
    "        bucket_ece /= len(abstain_scores)\n",
    "    else:\n",
    "        bucket_ece = None\n",
    "\n",
    "    # abstain rate: what percentage of questions the LLM abstained from\n",
    "    abstain_rate = (B + D) / (A + B + C + D)\n",
    "            \n",
    "    return {\n",
    "        'reliable_accuracy': f\"{reliable_accuracy:.2f}\",\n",
    "        'effective_reliability': f\"{effective_reliability:.2f}\",\n",
    "        'abstain_accuracy': f\"{abstain_accuracy:.2f}\",\n",
    "        'abstain_precision': abstain_precision,\n",
    "        'abstain_recall': abstain_recall,\n",
    "        'abstain_f1': f\"{2 * abstain_precision * abstain_recall / (abstain_precision + abstain_recall):.2f}\" if abstain_precision is not None and abstain_recall is not None and (abstain_precision + abstain_recall) > 0 else None,\n",
    "        'abstain_ece': bucket_ece,\n",
    "        'abstain_rate': abstain_rate\n",
    "    }\n",
    "\n",
    "# --- 3. Answer Parsing ---\n",
    "def answer_parsing(response):\n",
    "    # mode 1: answer directly after\n",
    "    temp = response.strip().split(\" \")\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "        if option in temp[0]:\n",
    "            return option\n",
    "    # mode 2: \"The answer is A/B/C/D/E\"\n",
    "    temp = response.lower()\n",
    "    for option in [\"a\", \"b\", \"c\", \"d\", \"e\"]:\n",
    "        if \"the answer is \" + option in temp:\n",
    "            return option.upper()\n",
    "    # mode 3: \"Answer: A/B/C/D/E\"\n",
    "    temp = response.lower()\n",
    "    for option in [\"a\", \"b\", \"c\", \"d\", \"e\"]:\n",
    "        if \"answer: \" + option in temp:\n",
    "            return option.upper()\n",
    "    # mode 4: \" A/B/C/D/E \" or \" A/B/C/D/E.\"\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "        if \" \" + option + \" \" in response or \" \" + option + \".\" in response:\n",
    "            return option\n",
    "    # mode 5: \"The correct answer is A/B/C/D/E\"\n",
    "    temp = response.lower()\n",
    "    for option in [\"a\", \"b\", \"c\", \"d\", \"e\"]:\n",
    "        if \"the correct answer is \" + option in temp:\n",
    "            return option.upper()\n",
    "    # mode 6: \"A: \" or \"B: \" or \"C: \" or \"D: \" or \"E: \"\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "        if option + \": \" in response:\n",
    "            return option\n",
    "    # mode 7: \"A/B/C/D/E\" and EOS\n",
    "    try:\n",
    "        for option in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "            if option + \"\\n\" in response or response[-1] == option:\n",
    "                return option\n",
    "    except:\n",
    "        pass\n",
    "    # fail to parse\n",
    "    print(\"fail to parse answer\", response, \"------------------\")\n",
    "    return \"Z\" # so that its absolutely wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2660d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    if \"mistral\" in model_name.lower():\n",
    "        model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # Load in 16-bit to save memory, map to auto (GPU)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def format_input(tokenizer, prompt):\n",
    "    # Standardize input formatting\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # apply_chat_template handles the [INST] or specific tags automatically\n",
    "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text_input\n",
    "\n",
    "def process_single_item(model, tokenizer, prompt, correct_answer, layer_indices):\n",
    "    \"\"\"\n",
    "    Process one QA pair:\n",
    "    1. Extract Hidden States (Trajectory)\n",
    "    2. Generate Answer\n",
    "    3. Return CPU data and clean GPU\n",
    "    \"\"\"\n",
    "    # 1. Prepare Input\n",
    "    formatted_prompt = format_input(tokenizer, prompt)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 2. Forward Pass (Get Hidden States)\n",
    "    # We do this BEFORE generation to capture the \"reading\" state of the prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "    # Extract specific layers at the LAST token of the prompt\n",
    "    vectors = []\n",
    "    for idx in layer_indices:\n",
    "        # Shape: [batch, seq_len, hidden_dim] -> [hidden_dim]\n",
    "        # Move to CPU immediately to free VRAM\n",
    "        v = outputs.hidden_states[idx][:, -1, :].cpu().float().numpy().flatten()\n",
    "        vectors.append(v)\n",
    "    \n",
    "    # Concatenate vectors [layer_0, layer_mid, layer_last]\n",
    "    hidden_vec = np.concatenate(vectors)\n",
    "    \n",
    "    # 3. Generate Answer (Deterministic)\n",
    "    # We reuse 'inputs' so the context is identical\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10, \n",
    "            do_sample=False, # <--- IMPORTANT: Deterministic\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    response = tokenizer.decode(generated_ids[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    print(response)\n",
    "    # 4. Check Correctness\n",
    "    prediction = answer_parsing(response)\n",
    "    is_correct = 1 if prediction == correct_answer else 0\n",
    "    \n",
    "    # 5. Explicit Cleanup\n",
    "    del outputs\n",
    "    del generated_ids\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache() # Force VRAM release\n",
    "    \n",
    "    return hidden_vec, is_correct, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b788fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# You can change these variables directly\n",
    "MODEL_NAME = \"mistral\" # \"mistral\", \"llama2_7b\", etc.\n",
    "DATASET = \"mmlu\"       # \"mmlu\", \"hellaswag\", etc.\n",
    "PORTION = 1.0          # 1.0 for full dataset\n",
    "SEED = 42\n",
    "\n",
    "# Set Seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 1. Load Data\n",
    "print(f\"Loading Dataset: {DATASET}...\")\n",
    "with open(f\"data/{DATASET}.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    # Slice dataset based on portion\n",
    "    data[\"dev\"] = data[\"dev\"][:int(len(data[\"dev\"])*PORTION)]\n",
    "    data[\"test\"] = data[\"test\"][:int(len(data[\"test\"])*PORTION)]\n",
    "\n",
    "# 2. Load Model\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Define Layers to extract [Embedding, Middle, Last]\n",
    "num_layers = model.config.num_hidden_layers\n",
    "layer_indices = [0, num_layers // 2, num_layers]\n",
    "\n",
    "# 3. Processing Loop (Single Phase)\n",
    "# We store results in CPU lists (RAM)\n",
    "dev_embeddings = []\n",
    "dev_labels = [] # Correctness flags\n",
    "test_embeddings = []\n",
    "test_labels = [] # Correctness flags\n",
    "\n",
    "# Helper to construct prompt\n",
    "def make_prompt(d):\n",
    "    p = \"Question: \" + d[\"question\"] + \"\\n\"\n",
    "    for key in d[\"choices\"].keys():\n",
    "        p += (key + \": \" + d[\"choices\"][key] + \"\\n\")\n",
    "    p += \"Choose one answer from the above choices. The answer is\"\n",
    "    return p\n",
    "\n",
    "print(\"Processing Dev Set (Generating & Extracting)...\")\n",
    "for d in tqdm(data[\"dev\"]):\n",
    "    prompt = make_prompt(d)\n",
    "    vec, is_correct, _ = process_single_item(model, tokenizer, prompt, d[\"answer\"], layer_indices)\n",
    "    \n",
    "    dev_embeddings.append(vec)\n",
    "    dev_labels.append(is_correct)\n",
    "\n",
    "print(\"Processing Test Set (Generating & Extracting)...\")\n",
    "# Optional: To save to file immediately instead of RAM, use np.save inside loop\n",
    "# But accumulating in list is usually fine for <100k samples\n",
    "for d in tqdm(data[\"test\"]):\n",
    "    prompt = make_prompt(d)\n",
    "    vec, is_correct, _ = process_single_item(model, tokenizer, prompt, d[\"answer\"], layer_indices)\n",
    "    \n",
    "    test_embeddings.append(vec)\n",
    "    test_labels.append(is_correct)\n",
    "\n",
    "# 4. Clean up Model to free HUGE memory before training Linear Probe\n",
    "print(\"Unloading LLM...\")\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 5. Train Linear Probe\n",
    "print(\"Training Linear Probe...\")\n",
    "# Convert lists to Tensors\n",
    "X_train = torch.tensor(np.array(dev_embeddings), dtype=torch.float32)\n",
    "y_train = torch.tensor(dev_labels, dtype=torch.long)\n",
    "X_test = torch.tensor(np.array(test_embeddings), dtype=torch.float32)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Train\n",
    "input_dim = X_train.shape[1]\n",
    "probe = LinearModel(input_dim)\n",
    "probe = train_linear_model(probe, X_train, y_train, epochs=10)\n",
    "\n",
    "# 6. Evaluate & Metrics\n",
    "print(\"Evaluating...\")\n",
    "abstain_flags = []\n",
    "abstain_scores = []\n",
    "\n",
    "# Inference on Test Set\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    # Predict correctness\n",
    "    # We want to abstain if we think it's WRONG (Class 0)\n",
    "    # prediction 1 = confident correct, 0 = predict wrong (so abstain)\n",
    "    pred_class, prob_wrong = test_linear_model(probe, X_test[i].unsqueeze(0))\n",
    "    \n",
    "    # Note: Logic depends on how you define abstain_flag\n",
    "    # Usually: Abstain if model predicts \"Wrong\" (Class 0)\n",
    "    # abstain_flag = 1 means \"I choose to abstain\"\n",
    "    \n",
    "    # predicted_is_correct = pred_class.item() # 1 or 0\n",
    "    \n",
    "    # if predicted_is_correct == 1:\n",
    "    #     abstain_flags.append(0) # Do not abstain\n",
    "    # else:\n",
    "    #     abstain_flags.append(1) # Abstain\n",
    "\n",
    "    ABSTAIN_THRESHOLD = 0.5\n",
    "    if prob_wrong > ABSTAIN_THRESHOLD:\n",
    "        abstain_flags.append(1) # Abstain (Cautious)\n",
    "    else:\n",
    "        abstain_flags.append(0) # Answer (Brave)\n",
    "        \n",
    "    abstain_scores.append(prob_wrong) # Confidence of error\n",
    "\n",
    "# Calculate Metrics\n",
    "metrics_result = compute_metrics(test_labels, abstain_flags, abstain_scores)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Approach: Single Phase (Do_sample=False)\")\n",
    "print(\"Model:\", MODEL_NAME)\n",
    "print(\"Dataset:\", DATASET)\n",
    "print(metrics_result)\n",
    "print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
